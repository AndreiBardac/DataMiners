{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "A. Linear activation function cannot be used in a neural network to correctly classify data.\n",
    "\n",
    "B. Linear activation function cannot be used in a neural network to correctly classify data.\n",
    "\n",
    "C. According to Pinkus Theorem, it is possible to approximate any continuous function on a compact set using a neural network. However, this does not guarantee that a neural network, specifically a multilayer perceptron (MLP), can classify a linearly separable dataset with zero training error. Therefore, option E is incorrect.\n",
    "\n",
    "D. By process of elimination D is correct\n",
    "\n",
    "E. Option C is incorrect because it implies that a single-layer network is more economic than a multi-layer network, which is not necessarily the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Correct. number of weight parameters of the kernal in the convolution layer is the number of entries in size 5x5 matrix multiplied by the number of feature maps. So 5\\*5\\*16 = 400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
